{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example-based explanations\n",
    "- 이 방법은 특정한 dataset의 instance를 정해 모델을 해석하는 것이다.\n",
    "- Thing B is similar to thing A and A caused Y, so I predict that B will cause Y as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counterfactual Explanations\n",
    "- 해당 라이브러리는 없다고 한다. 그래서 기본적인 개념만 정리한다.\n",
    "- If X had not occurred, Y would not have occurred! 이러한 느낌이 'counterfactual'\n",
    "- 예를 들면, A가 아파트를 1억에 팔고 싶은데 9천밖에 값이 안된다. 1억을 만들기 위해 A는 '최소한의 노력'으로 (벽지바꾸기 등) 거의 근접하게 1억(desired outcome)을 만든다. 이러한 과정!\n",
    "\n",
    "### good counterfactual explanation?\n",
    "- counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\n",
    "- counterfactuals 는 꼭 train data에 있는 것이 아니여도 된다.\n",
    "- counterfactual instance should have feature values that are likely\n",
    "- counterfactual instance produces the predefined prediction as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gernerating conterfactual explanations\n",
    "- loss를 계산한다. loss는 counterfactual의 outcome과 원래  desierd outcome의 차이 + counterfactual과 instance of interest로 계산한다.\n",
    "- 최적화 방법에는 다양한 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래의 loss를 최적화 하면 됨\n",
    "$$L(x,{ x }^{ ' },{ y }^{ ' },\\Lambda ) = \\Lambda \\cdot {(   \\hat { f }({ x }^{ ' })-{ y }^{ ' } )  }^{2  }+d(x,{ x }^{ ' })$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  first term은 counterfactual x의 prediction과 desired outcome y hat의 거리 제곱\n",
    "- second term은 instance x to be explained 와 counterfactual x hat의 거리\n",
    "- parameter 람다가 클수록 desired outcome y hat과 가까운 counterfactual을 선호하고 반대는 x에 가까운 녀석을 선호\n",
    "- 최적화는 x의 distance는 최소화하고 람다는 최대화하는 적정지점을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 거리는 다음과 같다.\n",
    "$$d(x,{ x }^{ ' })\\quad =\\quad \\sum _{ j=1 }^{ p } \\frac { |{ x }_{ j } - { x }_{ j }^{'}| }{{MAD}_{j} }$$\n",
    "\n",
    "$${ MAD }_{ j }=\\quad { median }_{ i\\subseteq \\left\\{ 1,2...n \\right\\} } (|{x}_{i,j} -{ median }_{ l\\subseteq \\left\\{ 1,2...n \\right\\} } {x}_{l,j}|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장단점\n",
    "- 해석이 쉽다. 어떠한 가정과 숨겨진 의미 이런게 없다. LIME과 같은 위험성이 적다.\n",
    "- data or the model에 access가 불필요하다.data나 model에 대해서 밝힐 필요가 없다. outcom만 밝히면 된다.\n",
    "- 머신러닝을 사용안한다. 쉽다.\n",
    "- Rashomon effect가 일어날 수도...! 예를 들어, 우리가 10개의 counterfactuals를 구했다고 하자. 몇 개를 설명할 것이가? 무엇이 가장 좋은 것인가? 물론 여러개에서 좋은 선택 가능\n",
    "- 위의 내용은 범주형자료는 다루지 않는다.(A solution for only categorical features was proposed by Martens et. al\n",
    "(2014)⁹⁸)\n",
    "- 소프트웨어가 없다..! 하지만 곧 생길수도...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Examples\n",
    "- counterfactual과 비슷하다. 하지만 adversarial ex는 머신러닝 모델을 해석하는 것이 아니라 속이는데 목적이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### methods and examples\n",
    "- adversarial examples를 만드는 방법은 다양하다.\n",
    "- adversarial ex와 instance의 거리는 최소화 하면서 predictin을 desired adversarial outcome으로 바꾸는 것이 대부분 방법의 기본이다.\n",
    "- 여기서는 딥러닝을 이용한 이미지분류에 초점을 맞추고 있다.\n",
    "- 다양한 예시를 소개\n",
    "-  예를 들어, 판다를 분류하는 모델이 있다. adversarial ex를 만들어서 결과를 예측해보니 엉뚱한 것으로 예측했다. (사람이 보기에는 판다인게 확실한 data) 이에 대해 rubust한 모델을 만들기 위해 adversarial examples을 train data에 추가하라고 제안하기도 한다.\n",
    "- 이러한 것은 cyber attack과도 연결점이 있다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protoypes and Criticisms\n",
    "- prototype는 모든 데이터를 잘 나타내는 a data instance라고 한다.\n",
    "- criticism은 prototype으로 잘 나타내지 못하는 a data instance라고 한다.\n",
    "- prototype은 찾는 방법은 다양하다. 그 중 하나가 k-medoids로 kmeans와 관련이 있는 알고리즘이다. 군집화 알고리즘은 대부분 cluster center를 찾는데 이는 prototype으로 이용할 수 있다. 하지만 criticism은 찾아주지 않는다.\n",
    "- 이에 대해 MMD-critic이라는 방법론을 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD-critic\n",
    "\n",
    "Theory\n",
    "\n",
    "1.prototypes와 criticisms의 갯수를 정한다.\n",
    "\n",
    "2.data dist와 비슷한 protyotype dist가 만들어진다.\n",
    "\n",
    "3.prototype이 설명하지 못하는 나머지는 criticisms이 선택된다.\n",
    "\n",
    "MMD-critic을 위해 필요한 것\n",
    "- kernel function to estimate the data density\n",
    "- witness function too tell us how different two dist are at a particular data point\n",
    "- 둘 다 greedy search로 찾아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum mean discrepancy (MMD)\n",
    "- 두 분포간의 차이를 측정하는 것이다.\n",
    "- 목표는 두 분포의 차이를 최소화하는 것이다.\n",
    "- 아래의 식을 최소화! 하는 prototypes를 찾는다.\n",
    "$$MMD^{ 2 }=\\frac { 1 }{ m^{ 2 } } \\sum _{ i,j=1 }^{ m }{ k({ z }_{ i },{ z }_{ j }) } -\\frac { 2 }{ mn } \\sum _{ i,j=1 }^{ m,n }{ k({ z }_{ i },{ x }_{ j }) } -\\frac { 1 }{n^2  }\\sum _{i,j=1  }^{n  }{k({x}_{i},{x}_{j})}$$\n",
    "\n",
    "- k는 kernel function : mearsures the similarity of two points\n",
    "- m은 number of prototypes z, n은 number of data point in original dataset\n",
    "- the average proximity of the prototypes to each other와 the average proximity of the data points to each other)을 더하고  average proximity between the prototypes and all other data points (multiplied by 2)을 빼는 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding criticism\n",
    "\n",
    "$$witness(x)=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ k(x,{ z }_{ i }) } -\\frac { 1 }{ n } \\sum _{ j=1 }^{n }{ k(x,{ x }_{ j }) } $$\n",
    "\n",
    "- witness가 0에 가까울 수록 data와 prototypes의 density function이 비슷하다는 의미\n",
    "- 양수, 음수로 커질수록 ciriticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMD-critic을 어떻게 모델 해석에 사용?\n",
    "- 세가지로 요약할 수 있다.\n",
    "\n",
    "1.data dist에 대한 이해를 도와서\n",
    "\n",
    "2.interpretable model을 만들어서\n",
    "\n",
    "3.balck box model을 interpertable하게 해서\n",
    "\n",
    "- 예를 들어,prototype이랑 criticism으로 black box model을 prediction한다. 이를 분석한다. (분석과 해석이 쉬워진다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장단점\n",
    "- 알고리즘이 쉽다.\n",
    "- 모든 타입의 데이터와 머신러닝 모델에 적용이 가능하다.\n",
    "- prototype과 ciriticism의 갯수를 정해야한다.\n",
    "- 실행을 위한 코드가 있지만 nice한 패키지는 없다.\n",
    "- https://github.com/BeenKim/MMD-critic\n",
    "- protoypte을 찾는 간단한 방법 : k-medoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influential Instances\n",
    "- train data를 지웠을 때 모델의 prediction과 parameter에 큰 변화를 가져온다면 그 데이터를 'influential'하다고 한다.\n",
    "- 크게 두가지 방법으로 influential instance를 찾는다.\n",
    "- deletion diagnostics & influence function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근데 outlier와 influential instance가 상당히 헷갈린다. 맥락에 따라 동일한 녀석이 될 수도 있고 아닐 수도 있는듯 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 influential instance 모델 해석에 도움을 줄까?\n",
    "- 여러가지가 있겠지만 influential instance에 따라 모델에 큰 변화가 일어난다면 모델에 대한 신뢰가 떨어질 수도 있다. 아니면 influential instance 자체에 대한 해석을 할 수도 있을 것이다.\n",
    "- 그렇다면 이제 influential instance를 구하는 두 가지 방법에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion Diagnostics\n",
    "- 이미 통계적으로 수 많은 방법이 존재한다.\n",
    "\n",
    "DFBETA\n",
    "- instance를 지웠을 때 model parameter에 일으키는 변화를 측정\n",
    "- weight parameter가 있는 모델에만 해당(eg,regression )\n",
    "\n",
    "$${DFBETA}_{i}  =\\beta  - {  \\beta }^{ (-i) }$$\n",
    "- 베타는 vetor with model parameters이고 뒤에 값은 instance i를 빼고 train한 model parameter이다.\n",
    "\n",
    "Cook's distance\n",
    "- instance를 지웠을 때 model prediction에 일으키는 변화를 측정\n",
    "\n",
    "$${ D }_{ i }=\\frac { \\sum _{ i=1 }^{ n }{ ({ y }_{ j }-{ y }_{ j }^{ (-i) })^{ 2 } }  } { p\\cdot MSE } $$\n",
    "- (scaled) sum of the squared differences in the predicted outcome when the i-th instance is removed from the model\n",
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence Functions\n",
    "- a training instance에 모델의 파라미터와 예측값이 얼마나 depend하는지 측정\n",
    "- 수학적인 접근방법은 skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장단점\n",
    "- 머신러닝 모델을 디버깅하기 가장 좋은 방법중 하나이다.\n",
    "- Deletion diagnostic은 계산량이 크다.\n",
    "- influence function는 deletion diagnostic의 좋은 대안이지만 파라미터가 differentiable해야 사용가능하다.(tree-based model에는 사용할 수 없다)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
